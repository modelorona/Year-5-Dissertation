{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%history -g -f history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "from firebase_admin import credentials, firestore\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('paper')\n",
    "plt.style.use('seaborn-paper')\n",
    "\n",
    "def plot_bar(data, x_label, title, hue=None, save=False, prefix=''):\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    sc = sns.countplot(data=data, x=x_label, palette='colorblind', hue=hue)\n",
    "    sc.set(ylabel='Count')\n",
    "    if save:\n",
    "        plt.savefig('graphs/{}_bar_chart_{}.pdf'.format(prefix, title), dpi=300, bbox_inches='tight')\n",
    "    return plt\n",
    "\n",
    "\n",
    "def plot_line2(data, x, y, x_label, y_label, title, hue=None, ci=95, style=None, save=False, prefix=''):\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    sc = sns.lineplot(x=x, y=y, data=data, palette='colorblind', hue=hue, style=style, ci=ci)\n",
    "    sc.set(xlabel=x_label, ylabel=y_label)\n",
    "    if save:\n",
    "        plt.savefig('graphs/{}_plot_chart_{}.pdf'.format(prefix, title), dpi=300, bbox_inches='tight')\n",
    "    return plt\n",
    "\n",
    "\n",
    "def plot_line(data, x, y, x_label, y_label, title, x2=None, y2=None, y2_label=None, save=False, prefix=''):\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    sc = sns.lineplot(x=x, y=y, data=data, palette='colorblind', ci=None)\n",
    "    sc.set(xlabel=x_label, ylabel=y_label)\n",
    "    if x2 is not None:\n",
    "        anx = plt.twinx()\n",
    "        sc = sns.lineplot(x=x2, y=y2, data=data, palette='colorblind', color='r', ax=anx, ci=None)\n",
    "        sc.set(ylabel=y2_label)\n",
    "        anx.figure.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig('graphs/{}_plot_chart_{}.pdf'.format(prefix, title), dpi=300, bbox_inches='tight')\n",
    "    return plt\n",
    "\n",
    "\n",
    "def plot_hist(data, x, title, hue, save=False, prefix=''):\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    sc = sns.histplot(data=data, x=x, hue=hue, palette='colorblind', discrete=True, multiple='stack')\n",
    "    plt.xticks(range(1,10))\n",
    "    if save:\n",
    "        plt.savefig('graphs/{}_hist_{}.pdf'.format(prefix, title), dpi=300, bbox_inches='tight')\n",
    "    return plt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T14:03:02.299727Z",
     "start_time": "2021-03-10T14:03:02.290316Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    cred = credentials.Certificate('./serviceAccount.json')\n",
    "    firebase_admin.initialize_app(cred)\n",
    "    db = firestore.client()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_updated_key(item_key, obj, doc):\n",
    "    for key, value in obj.items():\n",
    "        updated_key = \"{}_{}\".format(item_key, key)\n",
    "        doc[updated_key] = value\n",
    "\n",
    "\n",
    "def squash_doc(doc):\n",
    "    keys = ['validationMetrics', 'oobMetrics', 'trainingInfo']\n",
    "    for key in keys:\n",
    "        obj = doc.get(key, None)\n",
    "        if obj is not None:\n",
    "            set_updated_key(key, obj, doc)\n",
    "            del doc[key]\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models_csv = Path('./data/all_docs.csv')\n",
    "all_models_docs = list()\n",
    "all_models_pd = None\n",
    "\n",
    "if all_models_csv.is_file():\n",
    "    print('csv exists, using that')\n",
    "    all_models_pd = pd.read_csv(all_models_csv, parse_dates=[2])\n",
    "else:\n",
    "    print('csv does not exist, getting data from firestore')\n",
    "    doc_stream = db.collection(u'modelStats').stream()\n",
    "    for doc in doc_stream:\n",
    "        all_models_docs.append(squash_doc(doc.to_dict()))\n",
    "    all_models_pd = pd.DataFrame(all_models_docs)\n",
    "    all_models_pd.to_csv(all_models_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections_csv = Path('./data/all_connections.csv')\n",
    "connections_docs = list()\n",
    "connections_pd = None\n",
    "\n",
    "if connections_csv.is_file():\n",
    "    print('csv exists, using that')\n",
    "    connections_pd = pd.read_csv(connections_csv)\n",
    "else:\n",
    "    print('csv does not exist, getting data from firestore')\n",
    "    doc_stream = db.collection(u'connections').stream()\n",
    "    for doc in doc_stream:\n",
    "        connections_docs.append(doc.to_dict())\n",
    "    connections_pd = pd.DataFrame(connections_docs)\n",
    "    connections_pd.to_csv(connections_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dt(ts):\n",
    "    return pd.to_datetime(ts, utc=True)\n",
    "\n",
    "def to_dt2(ts):\n",
    "    return pd.to_datetime(ts, utc=True, unit='ms')\n",
    "\n",
    "def confusion_na(cm):\n",
    "    if cm.startswith('['):\n",
    "        return ''\n",
    "    return cm\n",
    "\n",
    "config_1_date = pd.to_datetime(np.datetime64('2021-03-03'), utc=True)\n",
    "config_1_date_end = pd.to_datetime(np.datetime64(1615547734696, 'ms'), utc=True)\n",
    "\n",
    "config_2_date = pd.to_datetime(np.datetime64('2021-03-12'), utc=True)\n",
    "config_2_date_end = pd.to_datetime(np.datetime64(1616253120904, 'ms'), utc=True)\n",
    "\n",
    "config_3_date = pd.to_datetime(np.datetime64('2021-03-21'), utc=True)\n",
    "config_3_date_end = pd.to_datetime(np.datetime64(1617058800000, 'ms'), utc=True)\n",
    "\n",
    "def set_trial(ts, config_1_date, config_1_date_end, start2, end2, start3, end3):\n",
    "    if config_1_date <= ts <= config_1_date_end:\n",
    "        return 1\n",
    "    elif config_2_date <= ts <= config_2_date_end:\n",
    "        return 2\n",
    "    elif config_3_date <= ts <= config_3_date_end:\n",
    "        return 3\n",
    "    return 0\n",
    "\n",
    "def set_day_of_trial(row, config_1_date, config_2_date, config_3_date):\n",
    "    ts = None\n",
    "    if 'timestamp_utc' in row.index:\n",
    "        ts = row.timestamp_utc\n",
    "    else:\n",
    "        ts = row.event_timestamp\n",
    "    trial = row.trial\n",
    "    if ts is None: return -1\n",
    "    day = -1\n",
    "    if trial == 1:\n",
    "        day = (ts - config_1_date).days + 1\n",
    "    elif trial == 2:\n",
    "        day = (ts - config_2_date).days + 1\n",
    "    elif trial == 3:\n",
    "        day = (ts - config_3_date).days + 1\n",
    "    if day == 10: day = 9  # technically part of the same day, just the date was collected wrong for some reason\n",
    "    return day\n",
    "\n",
    "def set_config_num(trial):\n",
    "    if trial == 1:\n",
    "        return 3\n",
    "    elif trial == 2:\n",
    "        return 2\n",
    "    elif trial == 3:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections_pd['timestamp_utc'] = connections_pd.timestamp.apply(to_dt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections_pd['trial'] = connections_pd.timestamp_utc.apply(set_trial, args=(config_1_date, config_1_date_end, config_2_date, config_2_date_end, config_3_date, config_3_date_end))\n",
    "connections_pd = connections_pd[connections_pd.trial!=0]\n",
    "connections_pd['Trial Day'] = connections_pd.apply(set_day_of_trial, args=(config_1_date, config_2_date, config_3_date), axis=1)\n",
    "connections_pd = connections_pd[(connections_pd['Trial Day']>=1) & (connections_pd['Trial Day']<=9)]\n",
    "connections_pd['Configuration'] = connections_pd.trial.apply(set_config_num)\n",
    "connections_pd['Call'] = connections_pd.apply(lambda x: 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(connections_pd[['Trial Day', 'Call', 'Configuration']], 'Trial Day', 'WebRTC Connections on a Daily Basis', 'Configuration', save=True, prefix='perf').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance data\n",
    "performance_json_1 = Path('./data/performance.json')\n",
    "performance_json_2 = Path('./data/performance_2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_pd = pd.concat([pd.read_json(performance_json_1, lines=True), pd.read_json(performance_json_2, lines=True)])\n",
    "performance_pd.event_timestamp = performance_pd.event_timestamp.apply(to_dt)\n",
    "performance_pd = performance_pd[(performance_pd.event_timestamp>=config_1_date) & (performance_pd.event_timestamp<=config_3_date_end) & (performance_pd.app_display_version=='2.0')]\n",
    "performance_pd = performance_pd[performance_pd.trace_info.apply(lambda x: isinstance(x, dict))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_pd['trial'] = performance_pd.event_timestamp.apply(set_trial, args=(config_1_date, config_1_date_end, config_2_date, config_2_date_end, config_3_date, config_3_date_end))\n",
    "performance_pd = performance_pd[performance_pd.trial!=0]\n",
    "performance_pd['Trial Day'] = performance_pd.apply(set_day_of_trial, args=(config_1_date, config_2_date, config_3_date), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(performance_pd[performance_pd.event_name=='Trainer'].iloc[0].trace_info)\n",
    "print(performance_pd.iloc[0].trace_info)\n",
    "\n",
    "def trace_info_duration(trace_info):\n",
    "    return pd.to_timedelta('{}us'.format(trace_info.get('duration_us'))).total_seconds() * 1e3\n",
    "\n",
    "performance_pd['Duration (ms)'] = performance_pd.trace_info.apply(trace_info_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_info_metric(trace_info):\n",
    "    if 'metric_info' in trace_info:\n",
    "        return int(trace_info.get('metric_info').get('metric_value'))\n",
    "    return -1  # if it is -1, then there is no parent_trace_name as it is just a plain trace with no metric\n",
    "\n",
    "performance_pd['Metric Count'] = performance_pd.trace_info.apply(trace_info_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_pd['Configuration'] = performance_pd.trial.apply(set_config_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data_app_count_pd = performance_pd[performance_pd.event_name=='save_data_app_count']\n",
    "save_data_app_count_pd.trial.value_counts()\n",
    "plot_line2(save_data_app_count_pd, 'Trial Day', 'Duration (ms)', 'Trial Day', 'Duration (ms)', 'Average Time Spent Saving Data', 'Configuration', save=True, prefix='perf').show()\n",
    "plot_line2(save_data_app_count_pd[['Trial Day', 'Configuration', 'Metric Count']], 'Trial Day', 'Metric Count', 'Trial Day', 'Count', 'Average Amount of Applications Per Session', hue='Configuration', save=True, prefix='perf').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainModel_pd = performance_pd[performance_pd.event_name=='trainModel']\n",
    "trainModel_pd['Duration (s)'] = trainModel_pd['Duration (ms)'].apply(lambda x: x/1000)\n",
    "plot_line2(trainModel_pd, 'Trial Day', 'Duration (s)', 'Trial Day', 'Duration (s)', 'Average Time Spent on Model Training', hue='Configuration', save=True, prefix='perf').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "training_model_pd = performance_pd[(performance_pd.event_name=='trainModel') | (performance_pd.event_name=='setUpData') | (performance_pd.event_name=='Trainer')]\n",
    "training_model_pd['Duration (s)'] = training_model_pd['Duration (ms)'].apply(lambda x: x / 1000)\n",
    "training_model_pd['Task'] = training_model_pd.event_name.apply(lambda x: 'Model Training' if x=='trainModel' else ('Setup, Train, Export' if x=='Trainer' else 'Data Setup'))\n",
    "\n",
    "\n",
    "for config in range(1, 4):\n",
    "    export_avgs = list()\n",
    "    export_df = pd.DataFrame(['Export']*9)\n",
    "    config_df = pd.DataFrame([config]*9)\n",
    "    trial_day_df = pd.DataFrame([1,2,3,4,5,6,7,8,9])\n",
    "    config_data = training_model_pd[training_model_pd.Configuration==config]\n",
    "    for day in range(1, 10):\n",
    "        setup_avg = config_data[(config_data['Trial Day']==day) & (config_data.event_name=='setUpData')]['Duration (ms)'].mean()\n",
    "        trainModel_avg = config_data[(config_data['Trial Day']==day) & (config_data.event_name=='trainModel')]['Duration (ms)'].mean()\n",
    "        trainer_avg = config_data[(config_data['Trial Day']==day) & (config_data.event_name=='Trainer')]['Duration (ms)'].mean()\n",
    "        export_avgs.append(abs(trainer_avg - (setup_avg + trainModel_avg)))\n",
    "    df = pd.concat([df, pd.concat([pd.DataFrame(export_avgs), export_df, config_df, trial_day_df], axis=1, ignore_index=True)], ignore_index=True)\n",
    "\n",
    "df = df.rename(columns={0: 'Duration (ms)', 1: 'event_name', 2: 'Configuration', 3: 'Trial Day'})\n",
    "df['Duration (s)'] = df['Duration (ms)'].apply(lambda x: x/1000)\n",
    "df['Task'] = df.event_name\n",
    "\n",
    "plot_line2(df, 'Trial Day', 'Duration (s)', 'Trial Day', 'Duration (s)', 'Average Time Spent on Model Statistics Export', hue='Configuration', ci=95, save=True, prefix='perf').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_line2(pd.concat([training_model_pd, df]), 'Trial Day', 'Duration (s)', 'Trial Day', 'Duration (s)', 'Average Time Breakdowns for the Machine Learning Task', hue='Configuration', style='Task', ci=None, save=True, prefix='pref').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setUpData_pd = performance_pd[performance_pd.event_name=='setUpData']\n",
    "setUpData_pd['Duration (s)'] = setUpData_pd['Duration (ms)'].apply(lambda x: x/1000)\n",
    "plot_line2(setUpData_pd, 'Trial Day', 'Duration (s)', 'Trial Day', 'Duration (ms)', 'Average Time Spent on Data Setup for Model Training', hue='Configuration', save=True, prefix='perf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer_pd = performance_pd[performance_pd.event_name=='Trainer']\n",
    "Trainer_pd['Duration (s)'] = Trainer_pd['Duration (ms)'].apply(lambda x: x/1000)\n",
    "plot_line2(Trainer_pd, 'Trial Day', 'Duration (s)', 'Trial Day', 'Duration (s)', 'Average Time Spent on Model Prep, Training, and Export', 'Configuration', save=True, prefix='perf').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcObserverOnMessage_pd = performance_pd[performance_pd.event_name=='dcObserverOnMessage']\n",
    "plot_line2(dcObserverOnMessage_pd, 'Trial Day', 'Duration (ms)', 'Trial Day', 'Duration (ms)', 'Average Time Spent Handling WebRTC DataChannel Messages', 'Configuration', save=True, prefix='perf').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentry data\n",
    "sentry_csv = Path('./data/sentry_issues_export.csv')\n",
    "sentry_pd = pd.read_csv(sentry_csv)\n",
    "\n",
    "count_sum = sentry_pd['count'].sum()\n",
    "sentry_pd = sentry_pd.sort_values(by=['count'], ascending=False, ignore_index=True)\n",
    "print(count_sum)\n",
    "# top 10\n",
    "for i in range(10):\n",
    "    cur = sentry_pd.iloc[i]\n",
    "    print(cur)\n",
    "    print((cur['count'] / count_sum) * 100) \n",
    "    print('-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# survey data\n",
    "survey_pd = pd.read_csv('./data/survey.csv')\n",
    "print(survey_pd.columns)\n",
    "print('-------')\n",
    "\n",
    "print('avg age: ', survey_pd.Age.mean())\n",
    "print('std dev age: ', survey_pd.Age.std())\n",
    "print('min max age: ', survey_pd.Age.min(), survey_pd.Age.max())\n",
    "print('-------')\n",
    "\n",
    "survey_android_version = pd.Series([10, 8, 10, 9, 7, 11, 11, 10, 10, 10, 10, 10, 10, 11, 8, 11, 11, 11, 9, 10, 10, 10])\n",
    "print(survey_android_version.size)\n",
    "print(survey_android_version.value_counts(normalize=True).mul(100).round(1).astype(str) + '%')\n",
    "print('--------')\n",
    "\n",
    "survey_sias = survey_pd['Was your SIAS score above 33? (This can be found in the Settings, which can be found by clicking on the 3 dots on the top right of the screen when the app is launched)']\n",
    "print(survey_sias.value_counts(normalize=True).mul(100).round(1).astype(str) + '%')\n",
    "survey_android_manufacturer = pd.Series(['1plus', '1plus', '1plus', '1plus', 'samsung', 'samsung', 'samsung', 'samsung', 'samsung', 'samsung', 'samsung',\n",
    "                                         'xiaomi', 'xiaomi', 'xiaomi', 'xiaomi', 'xiaomi', 'pixel', 'pixel', 'pixel', 'moto', 'lg', 'huawei'])\n",
    "print('--------')\n",
    "print(survey_android_manufacturer.value_counts(normalize=True).mul(100).round(1).astype(str) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# survey data contd.\n",
    "epi_sharing_comfort = pd.DataFrame([4]*9 + [5]*5 + [3]*4 + [2]*3 + [1]*2)\n",
    "epi_sharing_comfort = epi_sharing_comfort.rename({0: 'Comfort Level (1=least, 5=very)'},axis=1)\n",
    "print(epi_sharing_comfort.value_counts(normalize=True).mul(100).round(2).astype(str) + '%')\n",
    "print('-----')\n",
    "\n",
    "future_epi_usage = pd.DataFrame(['Yes']*8 + ['No']*9 + ['Depends']*6)\n",
    "print(future_epi_usage.value_counts(normalize=True).mul(100).round(2).astype(str) + '%')\n",
    "print('-----')\n",
    "\n",
    "data_analysis_on_phone_comfort = pd.DataFrame([4]*11 + [5]*9 + [3]*2 + [2]*1)\n",
    "data_analysis_on_phone_comfort = data_analysis_on_phone_comfort.rename({0: 'Comfort Level (1=least, 5=very)'},axis=1)\n",
    "print(data_analysis_on_phone_comfort.value_counts(normalize=True).mul(100).round(2).astype(str) + '%')\n",
    "print('-----')\n",
    "\n",
    "usage_stats_comfort = pd.DataFrame([3]*7 + [2]*6 + [5]*5 + [4]*4 + [1])\n",
    "usage_stats_comfort = usage_stats_comfort.rename({0: 'Comfort Level (1=least, 5=very)'},axis=1)\n",
    "print(usage_stats_comfort.value_counts(normalize=True).mul(100).round(2).astype(str) + '%')\n",
    "print('-----')\n",
    "\n",
    "centralised_comfort = pd.DataFrame([3]*10 + [2]*5 + [4]*4 + [5]*2 + [1])\n",
    "centralised_comfort = centralised_comfort.rename({0: 'Comfort Level (1=least, 5=very)'},axis=1)\n",
    "print(centralised_comfort.value_counts(normalize=True).mul(100).round(2).astype(str) + '%')\n",
    "print('-----')\n",
    "\n",
    "fed_comfort = pd.DataFrame([4]*10 + [3]*9 + [5]*2 + [2]*1)\n",
    "fed_comfort = fed_comfort.rename({0: 'Comfort Level (1=least, 5=very)'},axis=1)\n",
    "print(fed_comfort.value_counts(normalize=True).mul(100).round(2).astype(str) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert timestamp\n",
    "all_models_pd['timestamp_utc'] = all_models_pd.timestamp.apply(to_dt)\n",
    "# remove those that were there before trial began\n",
    "all_models_pd = all_models_pd[all_models_pd.timestamp_utc>=config_1_date]\n",
    "all_models_pd = all_models_pd.drop(columns=['timestamp'])\n",
    "\n",
    "all_models_pd.confusionMatrix = all_models_pd.confusionMatrix.apply(confusion_na)\n",
    "all_models_pd = all_models_pd[all_models_pd.confusionMatrix!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = all_models_pd.columns.tolist()\n",
    "cols[-1],cols[1] = cols[1],cols[-1]\n",
    "all_models_pd = all_models_pd[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_overall_anxious(row):\n",
    "    return row.trainingInfo_anxiousCountTest + row.trainingInfo_anxiousCountTrain\n",
    "\n",
    "def count_overall_non_anxious(row):\n",
    "    return row.trainingInfo_testSize + row.trainingInfo_trainSize\n",
    "\n",
    "def count_overall_data_size(row):\n",
    "    return row.trainingInfo_anxiousCountTest + row.trainingInfo_anxiousCountTrain + row.trainingInfo_testSize + row.trainingInfo_trainSize\n",
    "\n",
    "all_models_pd['overall_anxious'] = all_models_pd.apply(count_overall_anxious, axis=1)\n",
    "all_models_pd['overall_data'] = all_models_pd.apply(count_overall_data_size, axis=1)\n",
    "all_models_pd['overall_non_anxious'] = all_models_pd.apply(count_overall_non_anxious, axis=1)\n",
    "\n",
    "all_models_pd['overall_anxious'] = all_models_pd['overall_anxious'].astype(int)\n",
    "all_models_pd['overall_data'] = all_models_pd['overall_data'].astype(int)\n",
    "all_models_pd['overall_non_anxious'] = all_models_pd['overall_non_anxious'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models_pd = all_models_pd[(all_models_pd.validationMetrics_f1score<=1.0) & (all_models_pd.validationMetrics_f1score>=0.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models_pd['trial'] = all_models_pd.timestamp_utc.apply(set_trial, args=(config_1_date, config_1_date_end, config_2_date, config_2_date_end, config_3_date, config_3_date_end))\n",
    "all_models_pd['trialDay'] = all_models_pd.apply(set_day_of_trial, args=(config_1_date, config_2_date, config_3_date), axis=1)\n",
    "\n",
    "all_models_pd = all_models_pd[(all_models_pd.trial!=0) & (all_models_pd.trialDay!=-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_1_data = all_models_pd[all_models_pd.trial==3]\n",
    "config_2_data = all_models_pd[all_models_pd.trial==2]\n",
    "config_3_data = all_models_pd[all_models_pd.trial==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_1_data_avgs = dict()\n",
    "for x in range(1,10):\n",
    "    c = config_1_data[config_1_data.modelType=='COMBINED']\n",
    "    c = c[c.trialDay==x]\n",
    "    config_1_data_avgs[x] = [\n",
    "        c.oobMetrics_accuracy.mean() * 100.000,\n",
    "        c.oobMetrics_accuracy.std() * 100.00,\n",
    "        c.overall_data.mean(),\n",
    "        c.overall_anxious.mean(),\n",
    "        c.overall_anxious.mean() / c.overall_data.mean() * 100.00,\n",
    "        c.validationMetrics_f1score.mean(),\n",
    "        c.validationMetrics_f1score.std()\n",
    "    ]\n",
    "\n",
    "config_1_data_avgs_pd = pd.DataFrame.from_dict(config_1_data_avgs, orient='index', columns=['accuracy', 'accuracy std', 'overall data', 'anxious data', '% anxious', 'f1', 'f1 std'])\n",
    "display(config_1_data_avgs_pd.T)\n",
    "display(config_1_data_avgs_pd.accuracy.pct_change())\n",
    "q = pd.DataFrame(config_1_data_avgs_pd.accuracy.pct_change())\n",
    "\n",
    "\n",
    "config_1_data_avgs = dict()\n",
    "for x in range(1, 10):\n",
    "    c = config_1_data[config_1_data.modelType=='DAILY']\n",
    "    c = c[c.trialDay==x]\n",
    "    config_1_data_avgs[x] = [\n",
    "        c.oobMetrics_accuracy.mean() * 100.000,\n",
    "        c.oobMetrics_accuracy.std() * 100.00,\n",
    "        c.overall_data.mean(),\n",
    "        c.overall_anxious.mean(),\n",
    "        c.overall_anxious.mean() / c.overall_data.mean() * 100.00,\n",
    "        c.validationMetrics_f1score.mean(),\n",
    "        c.validationMetrics_f1score.std()\n",
    "    ]\n",
    "\n",
    "config_1_data_avgs_pd = pd.DataFrame.from_dict(config_1_data_avgs, orient='index', columns=['accuracy', 'accuracy std', 'overall data', 'anxious data', '% anxious', 'f1', 'f1 std'])\n",
    "display(config_1_data_avgs_pd.T)\n",
    "display(config_1_data_avgs_pd.accuracy.pct_change())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_2_data_avgs = dict()\n",
    "for x in range(1, 10):\n",
    "    c = config_2_data[config_2_data.modelType=='COMBINED']\n",
    "    c = c[c.trialDay==x]\n",
    "    config_2_data_avgs[x] = [\n",
    "        c.oobMetrics_accuracy.mean() * 100.000,\n",
    "        c.oobMetrics_accuracy.std() * 100.00,\n",
    "        c.overall_data.mean(),\n",
    "        c.overall_anxious.mean(),\n",
    "        c.overall_anxious.mean() / c.overall_data.mean() * 100.00,\n",
    "        c.validationMetrics_f1score.mean(),\n",
    "        c.validationMetrics_f1score.std()\n",
    "    ]\n",
    "\n",
    "config_2_data_avgs_pd = pd.DataFrame.from_dict(config_2_data_avgs, orient='index', columns=['accuracy', 'accuracy std', 'overall data', 'anxious data', '% anxious', 'f1', 'f1 std'])\n",
    "display(config_2_data_avgs_pd.T)\n",
    "display(config_2_data_avgs_pd.accuracy.pct_change())\n",
    "q2 = config_2_data_avgs_pd.accuracy.pct_change()\n",
    "\n",
    "\n",
    "config_2_data_avgs = dict()\n",
    "for x in range(1, 10):\n",
    "    c = config_2_data[config_2_data.modelType=='DAILY']\n",
    "    c = c[c.trialDay==x]\n",
    "    config_2_data_avgs[x] = [\n",
    "        c.oobMetrics_accuracy.mean() * 100.000,\n",
    "        c.oobMetrics_accuracy.std() * 100.00,\n",
    "        c.overall_data.mean(),\n",
    "        c.overall_anxious.mean(),\n",
    "        c.overall_anxious.mean() / c.overall_data.mean() * 100.00,\n",
    "        c.validationMetrics_f1score.mean(),\n",
    "        c.validationMetrics_f1score.std()\n",
    "    ]\n",
    "\n",
    "config_2_data_avgs_pd = pd.DataFrame.from_dict(config_2_data_avgs, orient='index', columns=['accuracy', 'accuracy std', 'overall data', 'anxious data', '% anxious', 'f1', 'f1 std'])\n",
    "display(config_2_data_avgs_pd.T)\n",
    "display(config_2_data_avgs_pd.accuracy.pct_change())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_3_data_avgs = dict()\n",
    "for x in range(1, 10):\n",
    "    c = config_3_data[config_3_data.modelType=='COMBINED']\n",
    "    c = c[c.trialDay==x]\n",
    "    config_3_data_avgs[x] = [\n",
    "        c.oobMetrics_accuracy.mean() * 100.000,\n",
    "        c.oobMetrics_accuracy.std() * 100.00,\n",
    "        c.overall_data.mean(),\n",
    "        c.overall_anxious.mean(),\n",
    "        c.overall_anxious.mean() / c.overall_data.mean() * 100.00,\n",
    "        c.validationMetrics_f1score.mean(),\n",
    "        c.validationMetrics_f1score.std()\n",
    "    ]\n",
    "\n",
    "config_3_data_avgs_pd = pd.DataFrame.from_dict(config_3_data_avgs, orient='index', columns=['accuracy', 'accuracy std', 'overall data', 'anxious data', '% anxious', 'f1', 'f1 std'])\n",
    "display(config_3_data_avgs_pd.T)\n",
    "display(config_3_data_avgs_pd.accuracy.pct_change())\n",
    "\n",
    "config_3_data_avgs = dict()\n",
    "for x in range(1, 10):\n",
    "    c = config_3_data[config_3_data.modelType=='DAILY']\n",
    "    c = c[c.trialDay==x]\n",
    "    config_3_data_avgs[x] = [\n",
    "        c.oobMetrics_accuracy.mean() * 100.000,\n",
    "        c.oobMetrics_accuracy.std() * 100.00,\n",
    "        c.overall_data.mean(),\n",
    "        c.overall_anxious.mean(),\n",
    "        c.overall_anxious.mean() / c.overall_data.mean() * 100.00,\n",
    "        c.validationMetrics_f1score.mean(),\n",
    "        c.validationMetrics_f1score.std()\n",
    "    ]\n",
    "\n",
    "config_3_data_avgs_pd = pd.DataFrame.from_dict(config_3_data_avgs, orient='index', columns=['accuracy', 'accuracy std', 'overall data', 'anxious data', '% anxious', 'f1', 'f1 std'])\n",
    "display(config_3_data_avgs_pd.T)\n",
    "display(config_3_data_avgs_pd.accuracy.pct_change())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
